{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUwHyzc0cOBv"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit --quiet\n",
        "!pip install ntscraper\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Hate_Speech.py\n",
        "import streamlit as st\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import emoji\n",
        "from ntscraper import Nitter\n",
        "import pandas as pd\n",
        "import googleapiclient.discovery\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Add a background image using custom CSS\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    .stApp {\n",
        "        background-image: url('https://images.rawpixel.com/image_800/czNmcy1wcml2YXRlL3Jhd3BpeGVsX2ltYWdlcy93ZWJzaXRlX2NvbnRlbnQvbHIvdjU0NmJhdGNoMy1teW50LTM0LWJhZGdld2F0ZXJjb2xvcl8xLmpwZw.jpg');\n",
        "        background-size: cover;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "# Load the trained model\n",
        "model = joblib.load('/content/trained_model_rf.pkl')\n",
        "\n",
        "# Load the TF-IDF vectorizer\n",
        "tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer.pkl')\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removing HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Removing special symbols, hashtags, and other non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Removing punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    # Handling chat words\n",
        "    text = chat_convo(text)\n",
        "\n",
        "    # Handling emojis\n",
        "    text = demojize_text(text)\n",
        "\n",
        "    # Tokenization using spaCy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    tokens = [token.text for token in nlp(text)]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Define chat conversation replacements\n",
        "chat_words = {\n",
        "    \"lol\": \"laugh out loud\", \"brb\": \"be right back\", \"ttyl\": \"talk to you later\",\n",
        "    \"gtg\": \"got to go\", \"btw\": \"by the way\", \"omg\": \"oh my god\", \"idk\": \"i don't know\",\n",
        "    \"imho\": \"in my humble opinion\", \"icymi\": \"in case you missed it\", \"fyi\": \"for your information\",\n",
        "    \"smh\": \"shaking my head\", \"rofl\": \"rolling on the floor laughing\", \"dm\": \"direct message\",\n",
        "    \"dm me\": \"send me a direct message\",\"imo\": \"in my opinion\", \"tbh\": \"to be honest\",\n",
        "    \"dms open\": \"direct messages are open for communication\", \"lmao\": \"laughing my *a* off\",\n",
        "    \"fwiw\": \"for what it's worth\", \"afk\": \"away from keyboard\", \"asap\": \"as soon as possible\",\n",
        "    \"bff\": \"best friends forever\", \"faq\": \"frequently asked questions\", \"fyeo\": \"for your eyes only\",\n",
        "    \"jk\": \"just kidding\", \"nvm\": \"never mind\", \"otoh\": \"on the other hand\", \"tmi\": \"too much information\",\n",
        "    \"tyt\": \"take your time\", \"yolo\": \"you only live once\", \"wth\": \"what the heck\", \"roflmao\": \"rolling on the floor laughing my *a* off\",\n",
        "    \"til\": \"today i learned\", \"ootd\": \"outfit of the day\", \"nsfw\": \"not safe for work\", \"oot\": \"out of town\",\n",
        "    \"lolz\": \"laugh out loud, but with a 'z' for emphasis\", \"g2g\": \"got to go\"\n",
        "}\n",
        "\n",
        "# Define function to replace chat words\n",
        "def chat_convo(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        w = w.strip(string.punctuation)\n",
        "        if w.lower() in chat_words:\n",
        "            new_text.append(chat_words[w.lower()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "# Define function to handle emojis\n",
        "def demojize_text(text):\n",
        "    new_text = []\n",
        "    new_text.append(emoji.demojize(text))\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "def main():\n",
        "    # Sidebar with options\n",
        "    st.sidebar.title(\"Options\")\n",
        "    option = st.sidebar.radio(\"Choose an option\", [\"Home\", \"Tweets Analysis\", \"YouTube Comments Analysis\", \"File Upload\",\"About\"])\n",
        "\n",
        "    # Handle selected option\n",
        "    if option == \"Home\":\n",
        "        st.title(\"Detection of Hate Speech Against LGBT+ on Social Media\")\n",
        "        st.write(\"To detect and predict potential textual hate speech targetting people on their sexual orientation\")\n",
        "        st.write(\"Enter text in the box to check if it is Hate Speech\")\n",
        "        user_input = st.text_area(\"Enter text here\")\n",
        "        if st.button(\"Check for Hate Text\"):\n",
        "            prediction = predict(user_input)\n",
        "            if prediction == 0:\n",
        "                st.write(\"**Unlikely to be Hate Speech**\")\n",
        "            else:\n",
        "                st.write(\"**Potential Hate Speech**\")\n",
        "\n",
        "    if option == \"Tweets Analysis\":\n",
        "        st.title(\"Tweets Prediction for Textual Hate Speech\")\n",
        "        st.write(\"Enter term or a username to scrape tweets and predict for any potential hate speech\")\n",
        "        option = st.radio(\"Choose an option\", [\"Term\", \"Username\"])\n",
        "        if option == \"Term\":\n",
        "            term = st.text_input(\"Enter term\")\n",
        "        elif option == \"Username\":\n",
        "            username = st.text_input(\"Enter Twitter username\")\n",
        "        if st.button(\"Scrape Tweets\"):\n",
        "            if option == \"Term\":\n",
        "                if term.strip() == \"\":\n",
        "                    st.warning(\"Please enter a valid term.\")\n",
        "                else:\n",
        "                    scraper = Nitter()\n",
        "                    tweets = scraper.get_tweets(term, mode='term', number=10)\n",
        "            elif option == \"Username\":\n",
        "                if username.strip() == \"\":\n",
        "                    st.warning(\"Please enter a valid Twitter username.\")\n",
        "                else:\n",
        "                    scraper = Nitter()\n",
        "                    tweets = scraper.get_tweets(username, mode='user', number=10)\n",
        "            if tweets is not None:\n",
        "                tweet_texts = [tweet['text'] for tweet in tweets['tweets']]\n",
        "                predictions = []\n",
        "                for text in tweet_texts:\n",
        "                    prediction = predict(text)\n",
        "                    predictions.append(prediction)\n",
        "                df = pd.DataFrame({'Tweet': tweet_texts, 'Prediction': predictions})\n",
        "                st.write(\"Predictions for Tweets:\")\n",
        "                st.write(df)\n",
        "            else:\n",
        "                st.error(\"Failed to retrieve tweets. Please check the username and try again.\")\n",
        "\n",
        "    elif option == \"YouTube Comments Analysis\":\n",
        "        st.title(\"Youtube Comments Analysis\")\n",
        "        st.write(\"Enter a YouTube video link to scrape comments and make predictions\")\n",
        "        vid_input = st.text_area(\"Enter video link here\")\n",
        "        if st.button(\"Scrape comments\"):\n",
        "            if vid_input.strip() == \"\":\n",
        "                st.warning(\"Please enter a valid link\")\n",
        "            else:\n",
        "                dev = \"(paste your youtube API key here)\"  # YouTube API key\n",
        "                api_service_name = \"youtube\"\n",
        "                api_version = \"v3\"\n",
        "                DEVELOPER_KEY = dev\n",
        "                youtube = googleapiclient.discovery.build(api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
        "                from langdetect import detect  # Importing the detect function from langdetect library\n",
        "\n",
        "                def extract_comments(video, language='en'):\n",
        "                    request = youtube.commentThreads().list(part=\"snippet\", videoId=video, maxResults=100)\n",
        "                    comments = []\n",
        "                    response = request.execute()\n",
        "                    count=0\n",
        "                    f=0\n",
        "                    while True:\n",
        "                        for item in response['items']:\n",
        "                            comment = item['snippet']['topLevelComment']['snippet']\n",
        "                            public = item['snippet']['isPublic']\n",
        "                            comments.append([comment['textOriginal']])\n",
        "                            count=count+1\n",
        "                            if(count>100):\n",
        "                              f=1\n",
        "                              break\n",
        "                        if(f==1):\n",
        "                            break\n",
        "                        try:\n",
        "                            nextPageToken = response['nextPageToken']\n",
        "                        except KeyError:\n",
        "                            break\n",
        "                        nextRequest = youtube.commentThreads().list(part=\"snippet\", videoId=video, maxResults=100,\n",
        "                                                                    pageToken=nextPageToken)\n",
        "                        response = nextRequest.execute()\n",
        "                    st.write(comments)\n",
        "                    # Filter comments by language\n",
        "                    filtered_comments = []\n",
        "                    prediction = []\n",
        "                    for comment in comments:\n",
        "                        try:\n",
        "                            if detect(comment[0]) == language:\n",
        "                                val = predict(comment[0])\n",
        "                                filtered_comments.append(comment[0])\n",
        "                                prediction.append(val)\n",
        "                        except:\n",
        "                            continue\n",
        "                    df2 = pd.DataFrame({'Comment': filtered_comments, 'Prediction': prediction})\n",
        "                    return df2\n",
        "\n",
        "                df = extract_comments(vid_input)\n",
        "                st.write(df)\n",
        "    elif option==\"File Upload\":\n",
        "        st.title(\"Upload files for Prediction\")\n",
        "        st.write(\"Predict hate text from data present present in a .csv file\")\n",
        "        st.write(\"Make sure that the data is present in 'text' column\")\n",
        "        uploaded_file = st.file_uploader(\"Choose a file\", type=[\"csv\"])\n",
        "        if uploaded_file is not None:\n",
        "            df = pd.read_csv(uploaded_file, usecols=['text'])  # Read only the 'text' column\n",
        "            st.write(\"Preview of the 'text' column from the uploaded file:\")\n",
        "            st.write(df.head())\n",
        "            # Make predictions from the uploaded CSV file\n",
        "            predictions = []\n",
        "            for index, row in df.iterrows():\n",
        "                prediction = predict(row['text'])\n",
        "                predictions.append(prediction)\n",
        "\n",
        "            # Add predictions as a new column in the DataFrame\n",
        "            df['Prediction'] = predictions\n",
        "\n",
        "            # Display the DataFrame with predictions\n",
        "            st.write(\"Predictions for uploaded data:\")\n",
        "            st.write(df)\n",
        "    elif option==\"About\":\n",
        "        st.title(\"Detection of Hate Speech Against LGBT+ on Social Media\")\n",
        "        st.write(\"\"\"\n",
        "    It doesn't seem surprising that even today, there is a stigma around the LGBT+ community, and individuals often face bullying and shaming in their lives. Whether it's on social media platforms or other video streaming applications where they share content or express their thoughts, users may encounter hate speech and insensitive trolling in the comments section, which can significantly impact their morale. This application aims to provide a user-friendly interface for predicting hate speech, helping filter out offensive or hateful comments.\n",
        "\n",
        "    **About the Project:**\n",
        "\n",
        "    **Model Training:** I trained a machine learning model to detect hate against LGBTQIA+ individuals. Using a supervised learning approach, the model learned patterns from labeled data to make predictions on unseen text. We utilized a dataset containing examples of hate speech and non-hate texts related to LGBTQIA+ topics for model training.\n",
        "\n",
        "    **Classification:** The trained model performs binary classification, distinguishing between hate text and non-hate text. Hate speech includes any communication that disparages LGBTQIA+ individuals based on their sexual orientation, gender identity, or other related attributes, while non-hate speech comprises text that does not exhibit characteristics of hate speech.\n",
        "\n",
        "    **Text Preprocessing:** I preprocessed the data to retain essential text for training and testing while removing unnecessary elements. This included removing HTML tags, punctuation, stop words, converting text to lowercase, replacing emoticons with their textual meaning, handling special symbols, and tokenizing the data.\n",
        "\n",
        "    **Vectorization:** Text data was converted into numerical vectors using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization. TF-IDF assigns weights to words based on their frequency in a document relative to their frequency in the entire corpus, helping capture the importance of words in a document.\n",
        "\n",
        "    **Classifier Used:** Then ,I employed the Random Forest classifier, an ensemble learning method that builds multiple decision trees during training and combines their predictions to make a final prediction. Random Forest introduces randomness by selecting random subsets of the training data and features for each decision tree, reducing overfitting and improving the model's generalization performance.\n",
        "\n",
        "    **Data Scraping:** To enable hate speech prediction from widely used platforms like Twitter and YouTube, we included data scraping functionality in this project. We utilized the Python module 'ntscraper' for scraping tweets based on a username or keyword and the YouTube API for scraping comments from a video link.\n",
        "\n",
        "    **Conclusion:** This project aims to combat hate speech directed at the LGBTQIA+ community by providing a tool to identify and filter out offensive comments. By leveraging machine learning and data scraping techniques, we strive to create a safer and more inclusive online environment for all individuals, regardless of their sexual orientation or gender identity.\n",
        "    \"\"\")\n",
        "\n",
        "\n",
        "\n",
        "def predict(text):\n",
        "    # Preprocess the input text\n",
        "    tokens = preprocess_text(text)\n",
        "    # Convert tokens back to text\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    # Transform preprocessed text using the loaded TF-IDF vectorizer\n",
        "    text_vectorized = tfidf_vectorizer.transform([preprocessed_text])\n",
        "\n",
        "    # Make predictions using the loaded model\n",
        "    prediction = model.predict(text_vectorized)[0]\n",
        "\n",
        "    return prediction\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "UhvzH2d1cSnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "O_vTg3Nqcc4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run Hate_Speech.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "FGwMUDYZcV0Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}